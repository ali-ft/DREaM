{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IgCLqyRuiNy"
      },
      "outputs": [],
      "source": [
        "!pip install datasets scikit-learn lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nxpQLglu0qj"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qjs67ZZPu3j3"
      },
      "outputs": [],
      "source": [
        "label_mapping = { 0: 'Cause-Effect-e2-e1', 1: 'Cause-Effect-e1-e2', 2: 'Component-Whole', 3: 'Component-Whole', 4: 'Content-Container', 5: 'Content-Container', 6: 'Entity-Destination', 7: 'Entity-Destination', 8: 'Entity-Origin', 9: 'Entity-Origin', 10: 'Instrument-Agency', 11: 'Instrument-Agency', 12: 'Member-Collection', 13: 'Member-Collection', 14: 'Message-Topic', 15: 'Message-Topic', 16: 'Product-Producer', 17: 'Product-Producer', 18: 'Other' }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi-dPYTMu411"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import os\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "# ========== تنظیمات NLTK ==========\n",
        "def setup_nltk():\n",
        "    \"\"\"دانلود و تنظیم داده‌های مورد نیاز NLTK\"\"\"\n",
        "    try:\n",
        "        nltk_data_path = os.path.expanduser('~/nltk_data')\n",
        "        if not os.path.exists(nltk_data_path):\n",
        "            os.makedirs(nltk_data_path)\n",
        "\n",
        "        nltk.download('punkt', download_dir=nltk_data_path, quiet=True)\n",
        "        nltk.download('wordnet', download_dir=nltk_data_path, quiet=True)\n",
        "        nltk.download('omw-1.4', download_dir=nltk_data_path, quiet=True)\n",
        "        nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_path, quiet=True)\n",
        "\n",
        "        nltk.data.path.append(nltk_data_path)\n",
        "    except Exception as e:\n",
        "        print(f\"خطا در تنظیم NLTK: {str(e)}\")\n",
        "\n",
        "setup_nltk()\n",
        "\n",
        "# ========== توابع پردازش متن ==========\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        if pd.isna(text) or not str(text).strip():\n",
        "            return \"\"\n",
        "        text = str(text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        return text.lower().strip()\n",
        "\n",
        "    def lemmatize_text(self, text):\n",
        "        text = self.clean_text(text)\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        try:\n",
        "            tokens = nltk.word_tokenize(text)\n",
        "        except:\n",
        "            tokens = text.split()\n",
        "\n",
        "        lemmas = []\n",
        "        for token in tokens:\n",
        "            lemma = self.lemmatizer.lemmatize(token, pos='v')\n",
        "            lemma = self.lemmatizer.lemmatize(lemma, pos='n')\n",
        "            lemma = self.lemmatizer.lemmatize(lemma, pos='a')\n",
        "            lemmas.append(lemma)\n",
        "\n",
        "        return \" \".join(lemmas)\n",
        "\n",
        "# ========== پردازش داده‌ها ==========\n",
        "def process_data():\n",
        "    ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\", download_mode=\"force_redownload\")\n",
        "\n",
        "    train_data = ds[\"train\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # پردازش داده‌های تست\n",
        "    test_df = pd.read_excel('contextual_paired_entities_V040129.xlsx')\n",
        "    test_df['Sentence'] = test_df['Sentence'].fillna('')\n",
        "\n",
        "    # ایجاد پیش‌پردازشگر\n",
        "    txt_prs = TextPreprocessor()\n",
        "\n",
        "    # پردازش متن‌های train و test\n",
        "    train_texts = []\n",
        "    train_labels = []\n",
        "\n",
        "    label_mapping = {\n",
        "        0: 'Cause-Effect',\n",
        "        1: 'Cause-Effect',\n",
        "        2: 'Component-Whole',\n",
        "        3: 'Component-Whole',\n",
        "        18: 'Other'\n",
        "    }\n",
        "\n",
        "    for row, label in zip(train_data, train_data['relation']):\n",
        "        if label in label_mapping:\n",
        "            text = get_terms_between_entities(row['sentence'])\n",
        "            if text:\n",
        "                processed_text = txt_prs.lemmatize_text(text)\n",
        "                if processed_text:\n",
        "                    train_texts.append(processed_text)\n",
        "                    train_labels.append(label_mapping[label])\n",
        "\n",
        "    # پردازش داده‌های تست\n",
        "    test_df['cleaned_text'] = test_df['Sentence'].apply(\n",
        "        lambda x: txt_prs.lemmatize_text(get_terms_between_entities(x))\n",
        "    )\n",
        "    test_df = test_df[test_df['cleaned_text'] != \"\"]\n",
        "\n",
        "    # پردازش ترم‌های آنووا\n",
        "    wb = load_workbook('Anova Terms Class Labels.xlsx')\n",
        "    sheet_names = ['anova_terms_cause_effect', 'anova_terms_component_whole']\n",
        "    important_terms = {}\n",
        "\n",
        "    for sheet_name in sheet_names:\n",
        "        class_name = sheet_name.replace('anova_terms_', '').replace('_', '-')\n",
        "        df = pd.read_excel('Anova Terms Class Labels.xlsx', sheet_name=sheet_name)\n",
        "\n",
        "        # پردازش 35 ترم اول Lemma (بدون تکراری)\n",
        "        lemma_terms = df['Lemma'].dropna().apply(txt_prs.lemmatize_text).unique()[:35]\n",
        "        important_terms[class_name] = set(lemma_terms)\n",
        "\n",
        "    return train_texts, train_labels, test_df, important_terms\n",
        "\n",
        "def get_terms_between_entities(sentence):\n",
        "    try:\n",
        "        terms = re.search(r'<e1>.*?</e1>(.*?)<e2>.*?</e2>', sentence)\n",
        "        return terms.group(1).strip() if terms else \"\"\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "# ========== آموزش مدل ==========\n",
        "def train_and_predict(train_texts, train_labels, test_df, important_terms):\n",
        "    # ایجاد دیتافریم آموزشی\n",
        "    train_df = pd.DataFrame({\n",
        "        'Text': train_texts,\n",
        "        'Label': train_labels\n",
        "    })\n",
        "\n",
        "    # بالانس کردن داده‌های آموزشی\n",
        "    min_class_samples = train_df['Label'].value_counts().min()\n",
        "    balanced_train_df = pd.concat([\n",
        "        train_df[train_df['Label'] == label].sample(min_class_samples, random_state=42)\n",
        "        for label in train_df['Label'].unique()\n",
        "    ])\n",
        "\n",
        "    # چاپ اطلاعات داده‌های آموزشی و تست\n",
        "    print(\"\\nاطلاعات داده‌های آموزشی و تست:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"تعداد نمونه‌های آموزشی: {len(balanced_train_df)}\")\n",
        "    print(f\"تعداد نمونه‌های تست: {len(test_df)}\")\n",
        "    print(\"\\nتوزیع کلاس‌ها در داده آموزشی:\")\n",
        "    print(balanced_train_df['Label'].value_counts())\n",
        "\n",
        "    # آماده‌سازی داده‌ها\n",
        "    all_terms = set()\n",
        "    for terms in important_terms.values():\n",
        "        all_terms.update(terms)\n",
        "\n",
        "    vectorizer = CountVectorizer(vocabulary=list(all_terms), ngram_range=(1, 2))\n",
        "    X_train = vectorizer.fit_transform(balanced_train_df['Text']).toarray()\n",
        "    y_train = balanced_train_df['Label']\n",
        "\n",
        "    # آموزش مدل\n",
        "    clf1 = RandomForestClassifier(random_state=42)\n",
        "    clf2 = GradientBoostingClassifier(random_state=42)\n",
        "    clf3 = LogisticRegression(random_state=42)\n",
        "    voting_clf = VotingClassifier(estimators=[('rf', clf1), ('gb', clf2), ('lr', clf3)], voting='soft')\n",
        "    voting_clf.fit(X_train, y_train)\n",
        "\n",
        "    # پیش‌بینی روی داده تست\n",
        "    X_test = vectorizer.transform(test_df['cleaned_text']).toarray()\n",
        "    test_df['Predicted_Label'] = voting_clf.predict(X_test)\n",
        "    test_df['Prediction_Probability'] = voting_clf.predict_proba(X_test).max(axis=1)\n",
        "    test_df['Predicted_Label'] = test_df['Predicted_Label'].apply(\n",
        "        lambda x: 'others' if x == 'Other' else x)\n",
        "\n",
        "    # چاپ اطلاعات پیش‌بینی\n",
        "    print(\"\\nتوزیع کلاس‌ها در پیش‌بینی داده تست:\")\n",
        "    print(test_df['Predicted_Label'].value_counts())\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # ذخیره نتایج\n",
        "    test_df.to_excel('output_040225.xlsx', index=False)\n",
        "\n",
        "# اجرای اصلی\n",
        "if __name__ == \"__main__\":\n",
        "    train_texts, train_labels, test_df, important_terms = process_data()\n",
        "    train_and_predict(train_texts, train_labels, test_df, important_terms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XILSwCdvbyL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import inflect\n",
        "import re\n",
        "\n",
        "# Load the Excel file\n",
        "excel_file = 'output_040225.xlsx'\n",
        "df = pd.read_excel(excel_file)\n",
        "\n",
        "# Extract the entities and labels\n",
        "df['Entity1'] = df['e1']\n",
        "df['Entity2'] = df['e2']\n",
        "\n",
        "# Filter out rows where Entity1 or Entity2 is empty\n",
        "df = df.dropna(subset=['Entity1', 'Entity2'])\n",
        "\n",
        "# Initialize inflect engine\n",
        "p = inflect.engine()\n",
        "\n",
        "# Normalize entities to singular form\n",
        "def normalize_entity(entity):\n",
        "    words = entity.split()  # بررسی تعداد کلمات در Entity\n",
        "    if len(words) > 1:  # اگر شامل چند کلمه باشد، آن را دست‌نخورده نگه داریم\n",
        "        return entity\n",
        "    singular = p.singular_noun(entity)\n",
        "    return singular if isinstance(singular, str) else entity  # فقط اسم‌های تکی را نرمال کنیم\n",
        "\n",
        "# Apply normalization safely\n",
        "df['Entity1'] = df['Entity1'].apply(normalize_entity)\n",
        "df['Entity2'] = df['Entity2'].apply(normalize_entity)\n",
        "\n",
        "# Clean Sentence column by removing specified tags\n",
        "df['Cleaned_Sentence'] = df['Sentence'].apply(lambda x: re.sub(r'</?e\\d?>', '', x) if pd.notna(x) else x)\n",
        "# df['Cleaned_Sentence'] = df['cleaned_text']\n",
        "\n",
        "# Select relevant columns for Cytoscape and rename columns to source and target\n",
        "cytoscape_df = df[['Cleaned_Sentence', 'Entity1', 'Entity2', 'Predicted_Label']]\n",
        "cytoscape_df.columns = ['sentence', 'source', 'target', 'label']\n",
        "\n",
        "# Filter out rows where label is 'others'\n",
        "cytoscape_df = cytoscape_df[cytoscape_df['label'] != 'others']\n",
        "\n",
        "# Save the data to a CSV file\n",
        "cytoscape_df.to_csv('data_drugs_Betweens_twoClass_V040225.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL7vYNuZwYGP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import openai\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    api_key='my-key'\n",
        ")\n",
        "\n",
        "# تابع اصلی برای تشخیص رابطه\n",
        "def detection_with_llm(ent1, ent2, sentence, mode='2class'):\n",
        "    if mode == '2class':\n",
        "        prompt = f\"\"\"\n",
        "Determine which of the two following semantic relationships best describes the relationship between the two highlighted drug entities in the sentence.\n",
        "\n",
        "Available relation types:\n",
        "\n",
        "1. cause-effect:\n",
        "   - Definition: One drug causes, leads to, or affects the function or effectiveness of the other.\n",
        "   - Example: \"Combining warfarin with aspirin can increase the risk of bleeding.\"\n",
        "     → warfarin <cause-effect> aspirin\n",
        "\n",
        "2. component-whole:\n",
        "   - Definition: One drug is a part or ingredient of a drug combination or formulation.\n",
        "   - Example: \"Vitamin B complex includes B1, B2, and B6.\"\n",
        "     → B1 <component-whole> Vitamin B complex\n",
        "\n",
        "Sentence: \"{sentence}\"\n",
        "Entity 1 (e1): \"{ent1}\"\n",
        "Entity 2 (e2): \"{ent2}\"\n",
        "\n",
        "Choose the most suitable label: cause-effect or component-whole.\n",
        "Only return one of these labels. No explanation needed.\n",
        "\"\"\"\n",
        "    elif mode == 'binary_causal':\n",
        "        prompt = f\"\"\"\n",
        "Check if the following sentence suggests a causal relationship between the two highlighted drug entities.\n",
        "\n",
        "Definition:\n",
        "- cause-effect: One drug causes, leads to, or affects the function, activity, or effect of the other.\n",
        "  Example: \"Taking rifampin can reduce the effectiveness of oral contraceptives.\"\n",
        "    → rifampin <cause-effect> oral contraceptives\n",
        "\n",
        "If such a causal relationship exists, return \"cause-effect\".\n",
        "Otherwise, return \"others\".\n",
        "\n",
        "Sentence: \"{sentence}\"\n",
        "Entity 1 (e1): \"{ent1}\"\n",
        "Entity 2 (e2): \"{ent2}\"\n",
        "\n",
        "Return only: cause-effect or others. No explanation.\n",
        "\"\"\"\n",
        "    else:\n",
        "        raise ValueError(\"Invalid mode selected. Choose '2class' or 'binary_causal'.\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model='gpt-4o-mini',\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": '''\n",
        "                You are an expert in biomedical language understanding. You carefully assess relationships between pharmaceutical entities based on the context of the sentence and your knowledge of drug interactions and classifications.\n",
        "            '''},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=100,\n",
        "        temperature=0,\n",
        "        timeout=120,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        llm_label = response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing response: {e}\")\n",
        "        llm_label = \"Error\"\n",
        "    print(prompt + '\\n')\n",
        "    time.sleep(1)\n",
        "    return llm_label.lower()\n",
        "\n",
        "# تابع اجرای اصلی\n",
        "def apply_stance_detection(input_path, output_path, mode):\n",
        "    df = pd.read_csv(input_path)\n",
        "    df['llm_label'] = None\n",
        "    for i, row in df.iterrows():\n",
        "        llm_label = detection_with_llm(row['source'], row['target'], row['sentence'], mode=mode)\n",
        "        df.at[i, 'llm_label'] = llm_label\n",
        "        print(f'Row {i}: {llm_label}\\n')\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"\\n✔ Results saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUwA5Hg6wcMa"
      },
      "outputs": [],
      "source": [
        "apply_stance_detection(\"data_drugs_Betweens_twoClass_V040225.csv\", \"result_data_drugs_Betweens_twoClass_V040225.csv\", \"2class\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMCTTK3ywg9c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "def plot_confusion_matrix(\n",
        "    csv_path,\n",
        "    label_col='label',\n",
        "    pred_col='llm_label',\n",
        "    classes=None,\n",
        "    title='Confusion Matrix',\n",
        "    figsize=(10, 8),\n",
        "    output_dir='./outputs',\n",
        "    save_plot=True,\n",
        "    save_report=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot a confusion matrix from a CSV file containing labels and predictions.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): Path to the CSV file.\n",
        "        label_col (str): Name of the column containing true labels.\n",
        "        pred_col (str): Name of the column containing predicted labels.\n",
        "        classes (list): List of possible class labels (if None, inferred from data).\n",
        "        title (str): Title of the plot.\n",
        "        figsize (tuple): Size of the figure (width, height).\n",
        "        output_dir (str): Directory to save outputs.\n",
        "        save_plot (bool): Whether to save the plot as PNG.\n",
        "        save_report (bool): Whether to save the classification report as TXT.\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Read CSV and preprocess labels\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df[label_col] = df[label_col].str.lower()\n",
        "    df[pred_col] = df[pred_col].str.lower()\n",
        "\n",
        "    # Define classes if not provided\n",
        "    if classes is None:\n",
        "        classes = sorted(set(df[label_col].unique()) | set(df[pred_col].unique()))\n",
        "\n",
        "    # Filter out invalid labels\n",
        "    valid_labels = set(classes)\n",
        "    df = df[df[label_col].isin(valid_labels) & df[pred_col].isin(valid_labels)]\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(df[label_col], df[pred_col], labels=classes)\n",
        "\n",
        "    # Plot heatmap\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=classes,\n",
        "        yticklabels=classes,\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    if save_plot:\n",
        "        plot_path = os.path.join(output_dir, 'confusion_matrix.png')\n",
        "        plt.savefig(plot_path, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Generate and save classification report\n",
        "    if save_report:\n",
        "        report = classification_report(\n",
        "            df[label_col],\n",
        "            df[pred_col],\n",
        "            target_names=classes,\n",
        "            zero_division=0,\n",
        "        )\n",
        "        report_path = os.path.join(output_dir, 'classification_report.txt')\n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write(report)\n",
        "        print(f\"✅ Classification report saved to: {report_path}\")\n",
        "\n",
        "    # Save confusion matrix as CSV\n",
        "    cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "    cm_csv_path = os.path.join(output_dir, 'confusion_matrix.csv')\n",
        "    cm_df.to_csv(cm_csv_path)\n",
        "    print(f\"✅ Confusion matrix saved to: {cm_csv_path}\\n\")\n",
        "\n",
        "    return cm_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpk2GmGywlvk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# کلاس‌های شما\n",
        "CLASSES = [\n",
        "    'cause-effect', 'component-whole'\n",
        "]\n",
        "\n",
        "# استخراج ماتریس سردرگمی\n",
        "cm_df = plot_confusion_matrix(\n",
        "    csv_path='result_data_drugs_Betweens_twoClass_V040225.csv',\n",
        "    label_col='llm_label',\n",
        "    pred_col='label',\n",
        "    classes=CLASSES,\n",
        "    title='Drug Relations Confusion Matrix (LLM vs Model)',\n",
        "    output_dir='./confusion_matrix_drug_betweens_twoClass_V040225',\n",
        ")\n",
        "\n",
        "# تبدیل ماتریس به edge list\n",
        "edges = []\n",
        "for i, source in enumerate(CLASSES):\n",
        "    for j, target in enumerate(CLASSES):\n",
        "        weight = cm_df.iloc[i, j]\n",
        "        if weight > 0:  # حذف مقادیر صفر\n",
        "            edges.append([source, target, weight])\n",
        "\n",
        "# تبدیل به DataFrame و ذخیره در CSV برای Cytoscape\n",
        "edges_df = pd.DataFrame(edges, columns=[\"source\", \"target\", \"label\"])\n",
        "edges_df.to_csv(\"confusion_matrix_twoClass_V040225.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMsPgzojwwfC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# خواندن فایل CSV\n",
        "file_path = \"result_data_drugs_Betweens_twoClass_V040225.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# فیلتر کردن داده‌ها\n",
        "df_cause_effect = df[(df[\"label\"] == \"Cause-Effect\") & (df[\"llm_label\"] == \"component-whole\")]\n",
        "df_others = df[(df[\"label\"] == \"Component-Whole\") & (df[\"llm_label\"] == \"cause-effect\")]\n",
        "\n",
        "# ایجاد یک فایل اکسل با دو شیت\n",
        "output_path = \"reports_twoClass_filtered_withoutner_all_3_V040225.xlsx\"\n",
        "\n",
        "with pd.ExcelWriter(output_path) as writer:\n",
        "    df_cause_effect.to_excel(writer, sheet_name=\"Cause-Effect\", index=False)\n",
        "    df_others.to_excel(writer, sheet_name=\"Component-Whole\", index=False)\n",
        "\n",
        "print(f\"✅ فایل {output_path} ایجاد شد!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
